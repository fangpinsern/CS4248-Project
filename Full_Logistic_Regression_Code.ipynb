{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full Logistic Regression Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python364jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
      "display_name": "Python 3.6.4 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AQuNFh7G0hX"
      },
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF2OcFn3HDRq",
        "outputId": "c048e3f8-cc31-4a2e-d98a-67d6b0b07231"
      },
      "source": [
        "#Downloads from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /usr/local/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /usr/local/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "datapath = 'si.csv'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIvR-e5hKpr3"
      },
      "source": [
        "def train_model(model, x_train, y_train):\n",
        "  logreg = model.fit(x_train, y_train)\n",
        "  print(\"Done Training\")\n",
        "  return logreg"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysIJG2_nKxK3"
      },
      "source": [
        "def predict(model, x_test):\n",
        "  pred = model.predict(x_test)\n",
        "  return pred"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs12HaUPKy8_"
      },
      "source": [
        "def generate_result(test, y_pred, filename):\n",
        "    ''' generate csv file base on the y_pred '''\n",
        "    test['pred'] = pd.Series(y_pred)\n",
        "    test.to_csv(filename, index=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf-fKYhMHL9P",
        "outputId": "dd80bfbb-5a6a-447b-a603-e8f11f38ebcc"
      },
      "source": [],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'BALANCED_train_test_split_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a46d335fb60c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset Imported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# vec = TfidfVectorizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BALANCED_train_test_split_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv(open(datapath, errors='ignore'))\n",
        "print(\"Dataset Imported\")\n",
        "X_train = train['headline'][train.phase == 'train']\n",
        "y_train = train['category'][train.phase == 'train']\n",
        "# vec = TfidfVectorizer()\n",
        "vec = CountVectorizer()\n",
        "X_train_Tfidf = vec.fit_transform(X_train)\n",
        "print(\"Done Vectorizing\")\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "print(\"Training model\")\n",
        "train_model(model, X_train_Tfidf, y_train)\n",
        "print(\"Done Training\")\n",
        "\n",
        "X_test = train['headline'][train.phase == 'test']\n",
        "X_test_Tfidf = vec.transform(X_test)\n",
        "y_pred = predict(model, X_test_Tfidf)\n",
        "test = train[train.phase == 'test']\n",
        "y_test = train['category'][train.phase == 'test']\n",
        "score = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('score on validation = {}'.format(score))\n",
        "print('score on accuracy = {}'.format(acc))\n",
        "generate_result(test, y_pred, \"RESULT.csv\")\n",
        "print(\"END\")"
      ]
    }
  ]
}