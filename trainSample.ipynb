{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd00f891fb4abc7e5b7925564c7fc55f147423425570aa5a97acbb1dd5ff2003f14",
   "display_name": "Python 3.7.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## LSTM with attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ngbra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ngbra\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from proj.data.data import *\n",
    "from proj.main import *\n",
    "from proj.constants import *\n",
    "from proj.models import all_tokenizers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "subset_df = pd.read_csv(os.path.join(DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "\n",
    "def split_col(df):\n",
    "    train = df[df['phase'] == 'train']\n",
    "    val = df[df['phase'] == 'dev']\n",
    "    test = df[df['phase'] == 'test']\n",
    "    return train, val, test\n",
    "\n",
    "dfs = split_col(subset_df)\n",
    "dls = []\n",
    "bs = 256\n",
    "model = \"lstmAttention\"\n",
    "tokenizer = None\n",
    "\n",
    "if model in all_tokenizers:\n",
    "    tokenizer = all_tokenizers[model]()\n",
    "\n",
    "for i, d in enumerate(dfs):\n",
    "    ds = NewsDataset(d, tokenizer=tokenizer)\n",
    "    sampler = get_weighted_sampler(ds.labels()) if i == 0 else None\n",
    "    dl = to_dataloader(ds, bs, sampler=sampler, drop_last=True)\n",
    "    dls.append(dl)\n",
    "\n",
    "hp = {**DEFAULT_HP, \"model\": model, \"lr\":2e-4, \"epochs\":10}\n",
    "trainer = Trainer(\"deep learning\", \"lstm_attn\", dls, hp, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_weights(\"lstm_balanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.opt.param_groups[0]['lr'] = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "trainer.scheduler.step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]epoch number: 0\n",
      "100%|██████████| 143/143 [00:22<00:00,  6.50it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6976169347763062, acc:0.7632211538461539, f1Score:0.7634022645767745\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7456220388412476, acc:0.71484375, f1Score:0.686134552925911\n",
      "epoch number: 1\n",
      "100%|██████████| 143/143 [00:21<00:00,  6.76it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6937229633331299, acc:0.7671820367132867, f1Score:0.7670261540831824\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7375907897949219, acc:0.7236328125, f1Score:0.6927312594523534\n",
      "epoch number: 2\n",
      "100%|██████████| 143/143 [00:21<00:00,  6.77it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6910780668258667, acc:0.7700502622377622, f1Score:0.7701585975469705\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.43it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.734884262084961, acc:0.7259765625, f1Score:0.6941888727232504\n",
      "epoch number: 3\n",
      "100%|██████████| 143/143 [00:21<00:00,  6.72it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6899276971817017, acc:0.7710882867132867, f1Score:0.771493055639946\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7321633100509644, acc:0.7291015625, f1Score:0.697764442613462\n",
      "epoch number: 4\n",
      "100%|██████████| 143/143 [00:20<00:00,  6.90it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6902728080749512, acc:0.7709790209790209, f1Score:0.7711013270452252\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.731796145439148, acc:0.7291015625, f1Score:0.6975105307469492\n",
      "epoch number: 5\n",
      "100%|██████████| 143/143 [00:20<00:00,  6.86it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6857272386550903, acc:0.7755681818181818, f1Score:0.7758562498734083\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7271044254302979, acc:0.7333984375, f1Score:0.7012161351639585\n",
      "epoch number: 6\n",
      "100%|██████████| 143/143 [00:20<00:00,  6.86it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.684226632118225, acc:0.7770705856643356, f1Score:0.7780700248578724\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.726043701171875, acc:0.7345703125, f1Score:0.702034249179696\n",
      "epoch number: 7\n",
      "100%|██████████| 143/143 [00:20<00:00,  6.85it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6868497133255005, acc:0.7743389423076923, f1Score:0.7747773044508584\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.43it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.726815104484558, acc:0.733984375, f1Score:0.701885407699679\n",
      "epoch number: 8\n",
      "100%|██████████| 143/143 [00:20<00:00,  6.89it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6862949132919312, acc:0.7750491695804196, f1Score:0.776147226911951\n",
      "100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7243050336837769, acc:0.736328125, f1Score:0.702947209920703\n",
      "epoch number: 9\n",
      "100%|██████████| 143/143 [00:21<00:00,  6.72it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6809800863265991, acc:0.7803212412587412, f1Score:0.7802276099769767\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7265907526016235, acc:0.7341796875, f1Score:0.7016504286347958\n",
      "100%|██████████| 41/41 [00:15<00:00,  2.61it/s]\n",
      "epoch test info: loss:1.7249369621276855, acc:0.7358041158536586, f1Score:0.7045581623193333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.one_cycle()"
   ]
  },
  {
   "source": [
    "### Balanced LSTM Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\ngbra\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from proj.data.data import *\n",
    "from proj.main import *\n",
    "from proj.constants import *\n",
    "from proj.models import all_tokenizers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "subset_df = pd.read_csv(os.path.join(DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "\n",
    "def split_col(df):\n",
    "    train = df[df['phase'] == 'train']\n",
    "    val = df[df['phase'] == 'dev']\n",
    "    test = df[df['phase'] == 'test']\n",
    "    return train, val, test\n",
    "\n",
    "dfs = split_col(subset_df)\n",
    "dls = []\n",
    "bs = 256\n",
    "model = \"lstm\"\n",
    "tokenizer = None\n",
    "\n",
    "if model in all_tokenizers:\n",
    "    tokenizer = all_tokenizers[model]()\n",
    "\n",
    "for i, d in enumerate(dfs):\n",
    "    ds = NewsDataset(d, tokenizer=tokenizer)\n",
    "    sampler = get_weighted_sampler(ds.labels()) if i == 0 else None\n",
    "    dl = to_dataloader(ds, bs, sampler=sampler, drop_last=True)\n",
    "    dls.append(dl)\n",
    "\n",
    "hp = {**DEFAULT_HP, \"model\": model, \"lr\":2e-4, \"epochs\":10}\n",
    "# trainer = Trainer(\"deep learning\", \"lstm_balanced\", dls, hp, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_weights(\"lstm_balanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.scheduler.gamma = 0.5\n",
    "trainer.opt.param_groups[0]['lr'] = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]epoch number: 0\n",
      "100%|██████████| 143/143 [00:16<00:00,  8.59it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.675702691078186, acc:0.7857025786713286, f1Score:0.7875331348136794\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7290502786636353, acc:0.7310546875, f1Score:0.6914131040822501\n",
      "epoch number: 1\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.75it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6732827425003052, acc:0.7883522727272727, f1Score:0.7897412882438359\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.39it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7258352041244507, acc:0.7349609375, f1Score:0.6943042482518397\n",
      "epoch number: 2\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.73it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6690218448638916, acc:0.792340472027972, f1Score:0.7935759035654084\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7253429889678955, acc:0.7341796875, f1Score:0.6931220193051575\n",
      "epoch number: 3\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.75it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6642441749572754, acc:0.7975579108391608, f1Score:0.7987570680198035\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7224416732788086, acc:0.73828125, f1Score:0.6986354316512056\n",
      "epoch number: 4\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.68it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6635007858276367, acc:0.7982408216783217, f1Score:0.7992692911074043\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7224098443984985, acc:0.7375, f1Score:0.6959123866390743\n",
      "epoch number: 5\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.70it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6622549295425415, acc:0.7995793269230769, f1Score:0.8011557385370814\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.48it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7212775945663452, acc:0.7390625, f1Score:0.6982140959976685\n",
      "epoch number: 6\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.83it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6621774435043335, acc:0.7996612762237763, f1Score:0.8009265267513485\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.45it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7203826904296875, acc:0.7396484375, f1Score:0.6990588825897582\n",
      "epoch number: 7\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.76it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6604808568954468, acc:0.8011363636363636, f1Score:0.8029233118668964\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7195453643798828, acc:0.7400390625, f1Score:0.6993385733428035\n",
      "epoch number: 8\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.79it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.658963680267334, acc:0.8027207167832168, f1Score:0.8041724837945855\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.45it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7197078466415405, acc:0.7400390625, f1Score:0.6991395792807185\n",
      "epoch number: 9\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.81it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.6605623960494995, acc:0.8011636800699301, f1Score:0.802133209099549\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.44it/s]\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7200580835342407, acc:0.7400390625, f1Score:0.6990562471724775\n",
      "100%|██████████| 41/41 [00:09<00:00,  4.52it/s]\n",
      "epoch test info: loss:1.7181484699249268, acc:0.7415205792682927, f1Score:0.7027354044075038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.one_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 41/41 [00:09<00:00,  4.47it/s]\n",
      "epoch test info: loss:1.7225946187973022, acc:0.7384717987804879, f1Score:0.6999938135651691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preds = trainer.getPreds(-1, False)\n",
    "testPreds = trainer.getPreds(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.main import *\n",
    "\n",
    "dfCopy[PRED_COL] = predCategories\n",
    "dfCopy[\"correct\"] = dfCopy[PRED_COL] == dfCopy[Y_COL]\n",
    "csvPath = os.path.join(\n",
    "    PREDS_DIR, f\"{trainer.model_name}_test_preds.csv\")\n",
    "dfCopy.to_csv(csvPath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCopy = dls[2].dataset.getDF()\n",
    "if len(testPreds) < len(dfCopy):\n",
    "    extra = len(dfCopy) - len(testPreds)\n",
    "    testPreds = torch.cat([testPreds, torch.tensor([-1] * extra)])\n",
    "predCategories = list(map(lambda l: CATEGORY_SUBSET[l], testPreds.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]epoch number: 0\n",
      "100%|██████████| 143/143 [00:13<00:00, 10.94it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7085702419281006, acc:0.7524857954545454, f1Score:0.7529745742038847\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.45it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.756931185722351, acc:0.703125, f1Score:0.6783157826457091\n",
      "epoch number: 1\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.57it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7104846239089966, acc:0.7502731643356644, f1Score:0.7502623627367463\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.51it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7268766164779663, acc:0.7333984375, f1Score:0.6884326424116878\n",
      "epoch number: 2\n",
      "100%|██████████| 143/143 [00:11<00:00, 12.00it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7139006853103638, acc:0.7474049388111889, f1Score:0.7479018842502474\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.52it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.8146213293075562, acc:0.6451171875, f1Score:0.6333897999363698\n",
      "epoch number: 3\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.89it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7238502502441406, acc:0.7368607954545454, f1Score:0.7381865287735621\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.52it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7925291061401367, acc:0.66796875, f1Score:0.6414481460965181\n",
      "epoch number: 4\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.72it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7076425552368164, acc:0.7535784527972028, f1Score:0.7543469825616971\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7528762817382812, acc:0.7064453125, f1Score:0.6733236918114002\n",
      "epoch number: 5\n",
      "100%|██████████| 143/143 [00:11<00:00, 12.00it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7013136148452759, acc:0.7593149038461539, f1Score:0.7599444738686559\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.51it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7356294393539429, acc:0.725, f1Score:0.6910563152843985\n",
      "epoch number: 6\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.48it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7044804096221924, acc:0.7566378933566433, f1Score:0.7565496852130986\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.38it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7364866733551025, acc:0.72421875, f1Score:0.6922810467952302\n",
      "epoch number: 7\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.56it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.7003238201141357, acc:0.7606260926573427, f1Score:0.7615971985689132\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.43it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7354968786239624, acc:0.72578125, f1Score:0.6951628581462762\n",
      "epoch number: 8\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.27it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.691996693611145, acc:0.7694219842657343, f1Score:0.7697739895480228\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.32it/s]\n",
      "  0%|          | 0/143 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7734934091567993, acc:0.68671875, f1Score:0.6735059102158164\n",
      "epoch number: 9\n",
      "100%|██████████| 143/143 [00:12<00:00, 11.33it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "epoch train info: loss:1.695694088935852, acc:0.764996722027972, f1Score:0.7659326289556624\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.38it/s]\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]\n",
      "epoch val info: loss:1.7335087060928345, acc:0.72578125, f1Score:0.6928251271556154\n",
      "100%|██████████| 41/41 [00:09<00:00,  4.36it/s]\n",
      "epoch test info: loss:1.7332167625427246, acc:0.7270388719512195, f1Score:0.6927846224258538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.one_cycle()"
   ]
  },
  {
   "source": [
    "## Bert Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from proj.data.data import *\n",
    "from proj.main import *\n",
    "from proj.constants import *\n",
    "from proj.models import all_tokenizers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "subset_df = pd.read_csv(os.path.join(DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "# subset_df = subset_df.iloc[:2000, :]\n",
    "\n",
    "def split_col(df):\n",
    "    train = df[df['phase'] == 'train']\n",
    "    val = df[df['phase'] == 'dev']\n",
    "    test = df[df['phase'] == 'test']\n",
    "    return train, val, test\n",
    "\n",
    "dfs = split_col(subset_df)\n",
    "dls = []\n",
    "bs = 256\n",
    "model = \"distilBert\"\n",
    "tokenizer = None\n",
    "\n",
    "if model in all_tokenizers:\n",
    "    tokenizer = all_tokenizers[model]()\n",
    "\n",
    "for i, d in enumerate(dfs):\n",
    "    ds = NewsDataset(d, tokenizer=tokenizer)\n",
    "    sampler = get_weighted_sampler(ds.labels()) if i == 0 else None\n",
    "    dl = to_dataloader(ds, bs, sampler=sampler, drop_last=False)\n",
    "    dls.append(dl)\n",
    "\n",
    "hp = {**DEFAULT_HP, \"model\": model, \"lr\":2e-4}\n",
    "trainer = Trainer(\"deep learning\", \"distilBert_og\", dls, hp, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_weights(\"distilBert_og.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NewsDataset(dfs[0], tokenizer=tokenizer)\n",
    "dl = to_dataloader(ds, bs, sampler=None, drop_last=False)\n",
    "trainer.dls.append(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 144/144 [00:40<00:00,  3.55it/s]\n",
      "\n",
      "epoch train info: loss:0.04168923571705818, acc:0.9861924913194444, f1Score:0.9889603945898806\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.getPreds(-1, toSave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/144 [00:00<?, ?it/s]epoch number: 0\n",
      "100%|██████████| 144/144 [01:02<00:00,  2.30it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "epoch train info: loss:0.06283114105463028, acc:0.9781901041666666, f1Score:0.9802947362823728\n",
      "100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]\n",
      "epoch val info: loss:0.7902320027351379, acc:0.8143601190476191, f1Score:0.7972760329498467\n",
      "epoch number: 1\n",
      "100%|██████████| 144/144 [01:02<00:00,  2.30it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "epoch train info: loss:0.0553152859210968, acc:0.9810655381944444, f1Score:0.9832098484763951\n",
      "100%|██████████| 21/21 [00:29<00:00,  1.41s/it]\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]\n",
      "epoch val info: loss:0.8516273498535156, acc:0.8223586309523809, f1Score:0.8086717794087275\n",
      "epoch number: 2\n",
      "100%|██████████| 144/144 [01:03<00:00,  2.28it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "epoch train info: loss:0.04697829484939575, acc:0.9836154513888888, f1Score:0.9858222504106312\n",
      "100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n",
      "\n",
      "epoch val info: loss:0.8356949090957642, acc:0.8290550595238095, f1Score:0.8101078670181261\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]epoch number: 3\n",
      "100%|██████████| 144/144 [01:03<00:00,  2.26it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "epoch train info: loss:0.04233165457844734, acc:0.9849446614583334, f1Score:0.9870380251402594\n",
      "100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n",
      "  0%|          | 0/144 [00:00<?, ?it/s]\n",
      "epoch val info: loss:0.7976826429367065, acc:0.8229166666666666, f1Score:0.8083003264060536\n",
      "epoch number: 4\n",
      "100%|██████████| 144/144 [01:02<00:00,  2.29it/s]\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\n",
      "epoch train info: loss:0.033077362924814224, acc:0.9881184895833334, f1Score:0.9902299173231809\n",
      "100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n",
      "  0%|          | 0/42 [00:00<?, ?it/s]\n",
      "epoch val info: loss:0.8704457879066467, acc:0.8258928571428571, f1Score:0.8094264532615337\n",
      "100%|██████████| 42/42 [00:31<00:00,  1.34it/s]\n",
      "epoch test info: loss:0.8840622901916504, acc:0.8182663690476191, f1Score:0.8022392296497938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.one_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "model = trainer.model\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=hp[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}