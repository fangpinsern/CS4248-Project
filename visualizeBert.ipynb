{
 "cells": [
  {
   "source": [
    "## Loading Data, models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.models import all_tokenizers\n",
    "from proj.constants import DATA_DIR\n",
    "from proj.data.data import split, split_col\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "subset_df = pd.read_csv(os.path.join(\n",
    "    DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "dfs = split_col(subset_df)\n",
    "\n",
    "headline = lambda x: dfs[2].headline.iloc[x]\n",
    "category = lambda x: dfs[2].category.iloc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import get_cmap\n",
    "\n",
    "CMAP = get_cmap(\"Blues\")\n",
    "\n",
    "def get_attentions(outputs, layer=5, attention_head=0, avg=True):\n",
    "  '''\n",
    "  get the particular output for a particular layer and attention head\n",
    "  layer -> 0 to 11\n",
    "  attention_head -> 0 to 11\n",
    "  '''\n",
    "  if avg:\n",
    "    #avg over all attention heads in a layer\n",
    "    returnVal =  outputs[layer].squeeze(0).mean(dim=0)\n",
    "    returnVal[1:] = 0\n",
    "    return returnVal\n",
    "\n",
    "  #return values for a particular attention head inside a specific layer\n",
    "  return outputs[layer].squeeze(0)[attention_head]\n",
    "\n",
    "def plt_attentions(mat, labs, fig_size=(8,8), annot=False, cmap = CMAP, title=None):\n",
    "  '''\n",
    "  plot the NxN matrix passed as a heat map\n",
    "  \n",
    "  mat: square matrix to visualize\n",
    "  labs: labels for xticks and yticks (the tokens in our case)\n",
    "  '''\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=fig_size) \n",
    "  plt.subplots_adjust(left=0.125, bottom=0.01, right=0.9, top=0.9)\n",
    "  ax = sns.heatmap(mat.detach().numpy(), annot=annot, yticklabels=labs,xticklabels=labs, cmap=cmap)\n",
    "  ax.xaxis.set_ticks_position('top')\n",
    "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "  if title:\n",
    "    ax.set_title(title)\n",
    "\n",
    "def attention_tokens(model, tokenizer, text):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    model.cpu()\n",
    "    attention = model(input_ids, attention_mask=mask)[-1]\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "    return attention, tokens\n",
    "\n",
    "def show_head_view(model, tokenizer, text, layer=5):\n",
    "    attention, tokens = attention_tokens(model, tokenizer, text)\n",
    "    head_view(attention, tokens, layer=layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from proj.data.data import NewsDataset\n",
    "from proj.models import all_models, all_tokenizers, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = all_tokenizers[\"distilBertPOS\"]()\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=10, output_attentions=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(torch.load(\"./proj/model_weights/60_20_20_split/distilBertPOS.pkl\"))\n",
    "transformerDS = NewsDataset(dfs[2], tokenizer=tokenizer, tag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_head_view(model, tokenizer, headline(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting TopK Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.data.data import *\n",
    "from proj.main import *\n",
    "from proj.models import all_tokenizers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_col(df):\n",
    "    train = df[df['phase'] == 'train']\n",
    "    val = df[df['phase'] == 'dev']\n",
    "    test = df[df['phase'] == 'test']\n",
    "    return train, val, test\n",
    "subset_df = pd.read_csv(os.path.join(\n",
    "    DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "dfs = split_col(subset_df)\n",
    "dls = []\n",
    "bs = 256\n",
    "model = \"distilBert\"\n",
    "tokenizer = None\n",
    "sampler = None\n",
    "\n",
    "if model in all_tokenizers:\n",
    "    tokenizer = all_tokenizers[model]()\n",
    "\n",
    "for i, d in enumerate(dfs):\n",
    "    ds = NewsDataset(d, tokenizer=tokenizer, stopwords=False)\n",
    "    sampler = get_weighted_sampler(ds.labels()) if i == 0 else None\n",
    "    dl = to_dataloader(ds, bs, sampler=sampler, drop_last=False)\n",
    "    dls.append(dl)\n",
    "model_name = \"distilBert_topK\"\n",
    "hp = {**DEFAULT_HP, \"model\": model, \"lr\":2e-4, \"epochs\":5}\n",
    "trainer = Trainer(\"_\", model_name, dls, hp, bs)\n",
    "trainer.load_weights(\"../ablation/distilbert_balanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = trainer.topKLoss(2, 100)"
   ]
  },
  {
   "source": [
    "## Visualization code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### POS WordCloud data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.data.data import *\n",
    "from proj.main import *\n",
    "from proj.models import all_tokenizers\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import torch \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "subset_df = pd.read_csv(os.path.join(\n",
    "    DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "dfs = split(subset_df)\n",
    "bs = 256\n",
    "modelName = \"distilBertPOS\"\n",
    "tokenizer = None\n",
    "sampler = None\n",
    "tokenizer = all_tokenizers[\"distilBertPOS\"]()\n",
    "allResults = []\n",
    "df = dfs[2]\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    for i, cat in enumerate(CATEGORY_SUBSET):\n",
    "        allAttention = []\n",
    "        allTokens = []\n",
    "        catDf = df[df.category == cat]\n",
    "        ds = NewsDataset(catDf, tokenizer=tokenizer, tag=True)\n",
    "        dl = to_dataloader(ds, bs, sampler=None, drop_last=False)\n",
    "        for xb, yb in tqdm(dl, total=len(dl)):\n",
    "            input_ids = xb[0]\n",
    "            attention = model(input_ids.cuda())[-1]\n",
    "            attention = attention[5].squeeze(0).mean(dim=1) # now a 16, 16 shape\n",
    "            # batch, 16, 16\n",
    "            clsAttention = attention[:, 0, :].cpu()\n",
    "            for ids in input_ids:\n",
    "                input_id_list = ids.cpu().tolist() # Batch index 0\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "                allTokens.append(tokens)\n",
    "            allAttention.append(clsAttention)\n",
    "        allAttention = torch.cat(allAttention)\n",
    "        allResults.append((allTokens, allAttention))"
   ]
  },
  {
   "source": [
    "#### no POS wordcloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.models import DistilBertForSequenceClassification\n",
    "\n",
    "subset_df = pd.read_csv(os.path.join(\n",
    "    DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "dfs = split(subset_df)\n",
    "\n",
    "dls = []\n",
    "bs = 256\n",
    "model = \"distilBert\"\n",
    "tokenizer = None\n",
    "sampler = None\n",
    "\n",
    "if model in all_tokenizers:\n",
    "    tokenizer = all_tokenizers[model]()\n",
    "\n",
    "for i, d in enumerate(dfs):\n",
    "    ds = NewsDataset(d, tokenizer=tokenizer)\n",
    "    sampler = get_weighted_sampler(ds.labels()) if i == 0 else None\n",
    "    dl = to_dataloader(ds, bs, sampler=sampler, drop_last=False)\n",
    "    dls.append(dl)\n",
    "model_name = \"_\"\n",
    "hp = {**DEFAULT_HP, \"model\": model, \"lr\":2e-4, \"epochs\":5}\n",
    "trainer = Trainer(\"60_20_20_split\", model_name, dls, hp, bs)\n",
    "trainer.load_weights(\"distilBert.pkl\")\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=10, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulating attention of a token across a dataset\n",
    "\n",
    "allResults = []\n",
    "df = dfs[2]\n",
    "# model = trainer.model\n",
    "with torch.no_grad():\n",
    "    for i, cat in enumerate(CATEGORY_SUBSET):\n",
    "        allAttention = []\n",
    "        allTokens = []\n",
    "        catDf = df[df.category == cat]\n",
    "        ds = NewsDataset(catDf, tokenizer=tokenizer, tag=True)\n",
    "        dl = to_dataloader(ds, bs, sampler=None, drop_last=False)\n",
    "        for xb, yb in tqdm(dl, total=len(dl)):\n",
    "            input_ids = xb[0]\n",
    "            attention = model(input_ids.cuda())[-1]\n",
    "            attention = attention[5].squeeze(0).mean(dim=1) # now a 16, 16 shape\n",
    "            # batch, 16, 16\n",
    "            clsAttention = attention[:, 0, :].cpu()\n",
    "            for ids in input_ids:\n",
    "                input_id_list = ids.cpu().tolist() # Batch index 0\n",
    "                tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "                allTokens.append(tokens)\n",
    "            allAttention.append(clsAttention)\n",
    "        allAttention = torch.cat(allAttention)\n",
    "        allResults.append((allTokens, allAttention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging attention of a token across a dataset\n",
    "\n",
    "finalData = []\n",
    "for i, cat in enumerate(CATEGORY_SUBSET):\n",
    "    attnDict = defaultdict(float)\n",
    "    freqDict = defaultdict(int)\n",
    "    data = allResults[i]\n",
    "    tokens, attentions = data\n",
    "    for j, currTokens in enumerate(tokens):\n",
    "        currAttns = attentions[j]\n",
    "        for k, tkn in enumerate(currTokens):\n",
    "            freqDict[tkn] += 1\n",
    "            attnDict[tkn] += currAttns[k]\n",
    "    for tkn in attnDict:\n",
    "        attnDict[tkn] = attnDict[tkn]/freqDict[tkn]\n",
    "        attnDict[tkn] = attnDict[tkn].tolist() * 10_000\n",
    "    finalData.append((freqDict, attnDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"./data/distilbertPOS_604040_attention_test_data.pkl\", \"wb\") as outfile:\n",
    "#     pickle.dump(finalData, outfile)\n",
    "\n",
    "# with open(\"./data/distilbert_604040_attention_test_data.pkl\", \"rb\") as infile:\n",
    "#     finalData = pickle.load(infile)\n",
    "\n",
    "# with open(\"./data/distilbertPOS_attention_test_data.pkl\", \"rb\") as infile:\n",
    "#     posFinalData = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIdx = -1\n",
    "sortedFirstCat = sorted(list(finalData[catIdx][1].items()), key=lambda d: d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "idx = 0\n",
    "print(CATEGORY_SUBSET[idx])\n",
    "freq = finalData[idx][1]\n",
    "wc = WordCloud( background_color=\"white\", width=1000, height=500)\n",
    "wc.generate_from_frequencies(frequencies=freq)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axes(title=\"no POS tag, CRIME category\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\", cmap=CMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = posFinalData[idx][1]\n",
    "wc = WordCloud( background_color=\"white\", width=1000, height=500)\n",
    "wc.generate_from_frequencies(frequencies=freq)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axes(title=\"with POS tag, CRIME category\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\", cmap=CMAP)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.models import all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.constants import DISTILBERT_POS_TOKENIZER\n",
    "from proj.models import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(DISTILBERT_POS_TOKENIZER, is_split_into_words=True)\n",
    "tokenizer.encode_plus(\"apologizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transformerDS.tokenize(headline(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from proj.constants import CATEGORY_SUBSET\n",
    "from proj.data.data import NewsDataset\n",
    "from proj.models import all_models, all_tokenizers, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = all_tokenizers[\"distilBert\"]()\n",
    "tfmer = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=10, output_attentions=True)\n",
    "# tfmer.resize_token_embeddings(len(tokenizer))\n",
    "tfmer.load_state_dict(torch.load(\"./proj/model_weights/ablation/distilbert_balanced.pkl\"))\n",
    "transformerDS = NewsDataset(dfs[2], tokenizer=tokenizer)\n",
    "dfs = split_col(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.data.data import *\n",
    "from proj.main import *\n",
    "from proj.models import all_tokenizers\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_col(df):\n",
    "    train = df[df['phase'] == 'train']\n",
    "    val = df[df['phase'] == 'dev']\n",
    "    test = df[df['phase'] == 'test']\n",
    "    return train, val, test\n",
    "subset_df = pd.read_csv(os.path.join(\n",
    "    DATA_DIR, \"train_test_split_dataset.csv\"))\n",
    "dfs = split_col(subset_df)\n",
    "dls = []\n",
    "bs = 128\n",
    "model = \"lstmAttention\"\n",
    "sampler = None\n",
    "\n",
    "if model in all_tokenizers:\n",
    "    tokenizer = all_tokenizers[model]()\n",
    "\n",
    "for i, d in enumerate(dfs):\n",
    "    ds = NewsDataset(d, tokenizer=None)\n",
    "    # sampler = get_weighted_sampler(ds.labels()) if i == 0 else None\n",
    "    dl = to_dataloader(ds, bs, sampler=sampler, drop_last=True)\n",
    "    dls.append(dl)\n",
    "model_name = \"lstmAttention_embed_train\"\n",
    "hp = {**DEFAULT_HP, \"model\": model, \"lr\":2e-4, \"epochs\":10}\n",
    "trainer = Trainer(\"padded\", model_name, dls, hp, bs)\n",
    "trainer.load_weights(f\"{model_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.attentionOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb, yb = next(iter(dls[2]))\n",
    "phase = 2\n",
    "ds = dls[phase].dataset\n",
    "dl = dls[phase]\n",
    "xb, yb = next(iter(dl))\n",
    "xb = xb[0].cuda(), xb[1]\n",
    "out, attn = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "packed = pack_padded_sequence(\n",
    "            xb[0], xb[1], batch_first=True, enforce_sorted=False)\n",
    "# Forward pass through LSTM\n",
    "# outputs, hidden = self.lstm(packed, self.hidden)\n",
    "padded = pad_packed_sequence(packed, batch_first=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 73, 71\n",
    "idx = 71\n",
    "tokenIds = ds[idx][0][0]\n",
    "tokens = []\n",
    "for tkid in tokenIds:\n",
    "    if tkid == 400001:\n",
    "        tokens.append(\"PAD\")\n",
    "        continue\n",
    "    if tkid >= 400000:\n",
    "        tokens.append(\"UNK\")\n",
    "        continue\n",
    "    tokens.append(ds.glove.itos[tkid])\n",
    "attnMap = attn[idx][0].cpu()\n",
    "attnMatrix = torch.cat([attnMap.unsqueeze(0), torch.zeros(15,16)])\n",
    "print(\"                predicted:\", CATEGORY_SUBSET[torch.argmax(out[idx]).cpu()],\",actual:\", CATEGORY_SUBSET[ds[idx][1]])\n",
    "plt_attentions(attnMatrix, tokens, title=\"LSTMAttention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmer = trainer.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topKIndices = topK.indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.data.data import NewsDataset\n",
    "# Use idx 0 for good comparison\n",
    "# idx = 7011\n",
    "transformerDS = NewsDataset(dfs[2], tokenizer=tokenizer)\n",
    "toTry = []\n",
    "\n",
    "lstIdx = 2\n",
    "for idx in topKIndices[lstIdx:lstIdx+1]:\n",
    "# for idx in range(6000,6100):\n",
    "    (inputIds, attention_mask), label = transformerDS[idx]\n",
    "    outputs = tfmer(\n",
    "        inputIds.unsqueeze(0),\n",
    "        attention_mask=attention_mask.unsqueeze(0)\n",
    "    )\n",
    "    pred = CATEGORY_SUBSET[torch.argmax(outputs[0].cpu())]\n",
    "    actual = category(idx)\n",
    "    if pred!=actual:\n",
    "        toTry.append(idx)\n",
    "    print(\"         predicted:\", CATEGORY_SUBSET[torch.argmax(outputs[0].cpu())],\",actual:\", category(idx))\n",
    "    text = \" \".join(transformerDS.tokenize(headline(idx)))\n",
    "    attention, tokens = attention_tokens(tfmer, tokenizer, transformerDS.tokenize(headline(idx)))\n",
    "    plt_attentions(get_attentions(attention), tokens, title=\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj.data.data import NewsDataset\n",
    "# Use idx 0 for good comparison\n",
    "# idx = 7011\n",
    "transformerDS = NewsDataset(dfs[2], tokenizer=tokenizer)\n",
    "toTry = []\n",
    "\n",
    "for idx in [6155]:\n",
    "    (inputIds, attention_mask), label = transformerDS[idx]\n",
    "    outputs = tfmer(\n",
    "        inputIds.unsqueeze(0),\n",
    "        attention_mask=attention_mask.unsqueeze(0)\n",
    "    )\n",
    "    pred = CATEGORY_SUBSET[torch.argmax(outputs[0].cpu())]\n",
    "    actual = category(idx)\n",
    "    if pred!=actual:\n",
    "        toTry.append(idx)\n",
    "    print(\"         predicted:\", CATEGORY_SUBSET[torch.argmax(outputs[0].cpu())],\",actual:\", category(idx))\n",
    "    text = \" \".join(transformerDS.tokenize(headline(idx)))\n",
    "    attention, tokens = attention_tokens(tfmer, tokenizer, transformerDS.tokenize(headline(idx)))\n",
    "    plt_attentions(get_attentions(attention), tokens, title=\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "from proj.data.data import NewsDataset\n",
    "from proj.models import all_models, all_tokenizers, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "dfs = split_col(subset_df)\n",
    "\n",
    "posTokenizer = all_tokenizers[\"distilBertPOS\"]()\n",
    "posTfmer = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=10, output_attentions=True)\n",
    "posTfmer.resize_token_embeddings(len(posTokenizer))\n",
    "# posTfmer.load_state_dict(torch.load(\"./proj/model_weights/POS/distilBert_pos.pkl\"))\n",
    "posTfmer.load_state_dict(torch.load(\"./proj/model_weights/60_20_20_split/distilBert_pos.pkl\"))\n",
    "posDS = NewsDataset(dfs[2], tokenizer=posTokenizer, tag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python378jvsc74a57bd034ef263a2ce6fd48a2918f04a9177354be61791929aa7041cb055017082594c5",
   "display_name": "Python 3.7.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}