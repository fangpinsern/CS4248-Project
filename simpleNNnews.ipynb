{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/eleazaroon/miniconda3/envs/cs4248/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "1859885it [02:18, 11250.41it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "em_dict = {}\n",
    "f = open('glove.840B.300d.txt', errors ='ignore', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    w = ''.join(values[:-300])\n",
    "    em_dict[w] = np.asarray(values[-300:], dtype=np.float32)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def break_hashtag(text):\n",
    "    text_words = re.split(r'(#\\w+)', text)\n",
    "    texts = []\n",
    "    for text_word in text_words:\n",
    "        if re.match(r'#\\w+', text_word):\n",
    "            words = []\n",
    "            i = 1\n",
    "            word = ''\n",
    "            while i < len(text_word):\n",
    "                if text_word[i].isupper():\n",
    "                    words.append(word)\n",
    "                    word = text_word[i]\n",
    "                else:\n",
    "                    word += text_word[i]\n",
    "                i += 1\n",
    "            words.append(word)\n",
    "            texts.append(' '.join(words).strip())\n",
    "        else:\n",
    "            texts.append(text_word)\n",
    "\n",
    "    return ' '.join(texts)\n",
    "\n",
    "def tokenize(text, with_stopwords=True):\n",
    "    text = break_hashtag(text)\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    if not with_stopwords:\n",
    "        s_tokens = [t for t in tokens if re.match(\n",
    "            r\"\\w+\", t) and t not in stop_words]\n",
    "        s_tokens = [lem.lemmatize(t) for t in s_tokens]\n",
    "        if len(s_tokens) > 0:\n",
    "            return s_tokens\n",
    "\n",
    "    # return [lem.lemmatize(t) for t in tokens]\n",
    "    return [t for t in tokens]\n",
    "\n",
    "def tokenize_synonyms(text):\n",
    "    synsets = []\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        synsetss = wn.synsets(token)\n",
    "        s_set = []\n",
    "        for s in synsetss:\n",
    "            s_set.append(s.lemmas()[0].name().lower())\n",
    "        s_set.sort()\n",
    "        if len(s_set) > 0:\n",
    "            synsets += s_set[0].split(\"_\")\n",
    "\n",
    "    return synsets\n",
    "\n",
    "def tokenize_hypernyms(text):\n",
    "    synsets = []\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        synsetss = wn.synsets(token)\n",
    "        h_set = []\n",
    "        for s in synsetss:\n",
    "            for h in s.hypernyms():\n",
    "                h_set.append(h.lemmas()[0].name().lower())\n",
    "\n",
    "        h_set.sort()\n",
    "        if len(h_set) > 0:\n",
    "            synsets += h_set[0].split(\"_\")\n",
    "            \n",
    "    return synsets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Bigram_Trigram_Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.bigram_trigram_vocab = pd.read_csv('bigram_trigram_vocab_PMI.csv')\n",
    "\n",
    "    def get_PMI_for_word(self, word):\n",
    "        pmi = self.bigram_trigram_vocab[self.bigram_trigram_vocab['ngram']\n",
    "                                        == word]['PMI'].values\n",
    "        if len(pmi) == 0:\n",
    "            return 0\n",
    "\n",
    "        return pmi[0]\n",
    "\n",
    "    def tokenize_with_bigrams(self, text):\n",
    "        unigrams = tokenize(text)\n",
    "        bigrams = [' '.join(t)\n",
    "                   for t in list(zip(unigrams, unigrams[1:]+[\" \"]))]\n",
    "        bigrams_pmi = [self.get_PMI_for_word(word) for word in bigrams]\n",
    "\n",
    "        def helper(left_start, right_end):\n",
    "            if left_start >= right_end:\n",
    "                return ''\n",
    "            max_bigram_arg = np.argmax(\n",
    "                bigrams_pmi[left_start:right_end]) + left_start\n",
    "            if bigrams_pmi[max_bigram_arg] > 0:\n",
    "                left = helper(left_start, max_bigram_arg)\n",
    "                right = helper(max_bigram_arg+2, right_end)\n",
    "                bi_unigram = '_'.join(bigrams[max_bigram_arg].split(' '))\n",
    "                return ' '.join([left, bi_unigram, right])\n",
    "            else:\n",
    "                return ' '.join(unigrams[left_start:right_end])\n",
    "\n",
    "        ret = helper(0, len(unigrams))\n",
    "        return nltk.word_tokenize(ret)\n",
    "\n",
    "    def get_bigram_trigram_token_list(self):\n",
    "        return self.bigram_trigram_vocab['token'].values\n",
    "\n",
    "    def get_bigram_token_list(self):\n",
    "        df = self.bigram_trigram_vocab.copy()\n",
    "        df['len'] = df['ngram'].apply(lambda x: len(x.split(' ')))\n",
    "        bigrams = df[df['len'] == 2]\n",
    "        return bigrams['token'].values\n",
    "\n",
    "    def get_bigram_glove_embeddings(self):\n",
    "        embeddings = pickle.load(open('mitten_bigram_dict_300d_515_10000.pkl', 'rb'))\n",
    "        return embeddings\n",
    "\n",
    "BigramTokenizer = Bigram_Trigram_Tokenizer()\n",
    "bigram_dict = BigramTokenizer.get_bigram_glove_embeddings()\n",
    "\n",
    "#Transform headline to Glove vectors. Different tokenize methods may be used.\n",
    "def preprocess_X(s):\n",
    "    matrix = []\n",
    "    words = str(s).lower()\n",
    "    wordsList = words\n",
    "    words = tokenize(words, with_stopwords=True)\n",
    "    #words = tokenize_hypernyms(wordsList)\n",
    "    #words = BigramTokenizer.tokenize_with_bigrams(wordsList)\n",
    "    \n",
    "    wordMatrix = []\n",
    "    for w in words:\n",
    "        if w in em_dict:\n",
    "            matrix.append(em_dict[w])\n",
    "            wordMatrix.append(w)\n",
    "    matrix = np.array(matrix)\n",
    "    agg = matrix.sum(axis=0)\n",
    "    \n",
    "    \n",
    "    return np.zeros(300) if type(agg) != np.ndarray else agg / np.sqrt((agg ** 2).sum())\n",
    "\n",
    "#Investigate the effects of Glove vectors on results\n",
    "def preprocess_X_test(s):\n",
    "    matrix = []\n",
    "    words = str(s).lower()\n",
    "    wordsList = words\n",
    "    words = tokenize(words)\n",
    "    wordMatrix = []\n",
    "    for w in words:\n",
    "        if w in em_dict:\n",
    "            matrix.append(em_dict[w])\n",
    "            wordMatrix.append(w)\n",
    "    originalMatrix = copy.deepcopy(matrix)\n",
    "    matrix = np.array(matrix)\n",
    "    agg = matrix.sum(axis=0)\n",
    "    if type(agg) == np.ndarray:\n",
    "        length = matrix.shape[1]\n",
    "        denom = np.sqrt((agg ** 2).sum())\n",
    "        i = 0\n",
    "        newAgg = []\n",
    "        for current in originalMatrix:\n",
    "            word = wordMatrix[i]\n",
    "            currentVector = current\n",
    "            current = current * length / denom\n",
    "            vA = current\n",
    "            vB = agg / denom\n",
    "            cos = np.dot(vA, vB) / (np.sqrt(np.dot(vA,vA)) * np.sqrt(np.dot(vB,vB)))\n",
    "            if cos >= 0.5:\n",
    "                newAgg.append(currentVector)\n",
    "            i = i + 1\n",
    "        newAgg = np.array(newAgg)\n",
    "        agg = newAgg.sum(axis = 0)\n",
    "    \n",
    "    return np.zeros(300) if type(agg) != np.ndarray else agg / np.sqrt((agg ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original imbalanced dataset\n",
    "\n",
    "df = pd.read_csv('train_test_split_dataset.csv')\n",
    "\n",
    "train = df.loc[df['phase'] == 'train']\n",
    "dev = df.loc[df['phase'] == 'dev']\n",
    "test = df.loc[df['phase'] == 'test']\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "y = lbl_enc.fit_transform(train.category.values)\n",
    "\n",
    "X_train = train.headline.values\n",
    "X_valid = dev.headline.values\n",
    "X_test = test.headline.values\n",
    "\n",
    "Y_train = lbl_enc.transform(train.category.values)\n",
    "Y_valid = lbl_enc.transform(dev.category.values)\n",
    "y_test_true = lbl_enc.transform(test.category.values)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Transform y labels from 0, 1, 2 to one-hot encoding\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "\n",
    "Y_valid = np_utils.to_categorical(Y_valid)\n",
    "\n",
    "# Perform preprocessing for x and transform to np array\n",
    "X_train = [preprocess_X(x) for x in tqdm(X_train)]\n",
    "X_train = np.array(X_train)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_valid = [preprocess_X(x) for x in tqdm(X_valid)]\n",
    "X_valid = np.array(X_valid)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(300, input_dim=300, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y=Y_train, batch_size=64,\n",
    "          epochs=100, verbose=1, validation_data=(X_valid, Y_valid), callbacks=[es])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.headline.values\n",
    "\n",
    "X_test = [preprocess_X(x) for x in tqdm(X_test)]\n",
    "X_test = np.array(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "print(y_test_pred)\n",
    "print(accuracy_score(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_test_true, y_test_pred,average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lbl_enc.inverse_transform(y_test_pred)\n",
    "\n",
    "test = test.assign(preds = y_test_pred)\n",
    "\n",
    "test.to_csv(\"simpleNN.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT_FILE = 'train_test_split_dataset.csv'\n",
    "CATEGORY_SUBSET = [\n",
    "    \"CRIME\",\n",
    "    \"RELIGION\",\n",
    "    \"TECH\",\n",
    "    \"MONEY\",\n",
    "    \"FOOD & DRINK\",\n",
    "    \"SPORTS\",\n",
    "    \"TRAVEL\",\n",
    "    \"WOMEN\",\n",
    "    \"STYLE\",\n",
    "    \"ENTERTAINMENT\",\n",
    "]\n",
    "import os\n",
    "def balanced_train_test_split(percent_train=0.7, percent_dev=0.1, percent_test=0.2, count=4000):\n",
    "    def balance_train_data(train_data, count):\n",
    "        ret = None\n",
    "        for cat in CATEGORY_SUBSET:\n",
    "            data_of_cat = train_data[train_data['category'] == cat]\n",
    "            data_of_cat = data_of_cat.sample(count, replace=True)\n",
    "            if ret is None:\n",
    "                ret = data_of_cat\n",
    "            else:\n",
    "                ret = pd.concat([ret, data_of_cat], axis=0)\n",
    "        return ret\n",
    "\n",
    "    if os.path.exists(TRAIN_TEST_SPLIT_FILE):\n",
    "        data = pd.read_csv(TRAIN_TEST_SPLIT_FILE)\n",
    "        train_data = data[data['phase']=='train']\n",
    "        other_data = data[data['phase']!='train']\n",
    "        train_data = balance_train_data(train_data, count)\n",
    "        data = pd.concat([train_data, other_data], axis=0)\n",
    "        return data\n",
    "\n",
    "    data = get_dataset()\n",
    "    l = len(data)\n",
    "    train_num = int(l*0.7)\n",
    "    dev_num = int(l*0.1)\n",
    "    rnd_ind = np.arange(l)\n",
    "    np.random.shuffle(rnd_ind)\n",
    "    train_ind = rnd_ind[:train_num]\n",
    "    dev_ind = rnd_ind[train_num:train_num+dev_num]\n",
    "    test_ind = rnd_ind[train_num+dev_num:]\n",
    "    data = data.reset_index()\n",
    "    data['ind'] = data.index\n",
    "\n",
    "    def change_phase(ind):\n",
    "        if ind in train_ind:\n",
    "            return 'train'\n",
    "        elif ind in dev_ind:\n",
    "            return 'dev'\n",
    "        else:\n",
    "            return 'test'\n",
    "\n",
    "    data['phase'] = data['ind'].apply(change_phase)\n",
    "    data = data.drop(columns=['ind'])\n",
    "\n",
    "    data.to_csv(TRAIN_TEST_SPLIT_FILE, index=False)\n",
    "\n",
    "    train_data = data[data['phase']=='train']\n",
    "    other_data = data[data['phase']!='train']\n",
    "    train_data = balance_train_data(train_data, count)\n",
    "    data = pd.concat([train_data, other_data], axis=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def new_balanced_train_test_split(percent_train=0.7, percent_dev=0.1, percent_test=0.2, count=4000):\n",
    "    def balance_train_data(train_data, count):\n",
    "        ret = None\n",
    "        for cat in CATEGORY_SUBSET:\n",
    "            data_of_cat = train_data[train_data['category'] == cat]\n",
    "            data_of_cat = data_of_cat.sample(count, replace=True)\n",
    "            if ret is None:\n",
    "                ret = data_of_cat\n",
    "            else:\n",
    "                ret = pd.concat([ret, data_of_cat], axis=0)\n",
    "        return ret\n",
    "\n",
    "    if os.path.exists(TRAIN_TEST_SPLIT_FILE):\n",
    "        data = pd.read_csv(TRAIN_TEST_SPLIT_FILE)\n",
    "        train_data = data[data['phase']=='train']\n",
    "        test_data = data[data['phase']=='test']\n",
    "        dev_data = data[data['phase']=='dev']\n",
    "        train_data = balance_train_data(train_data, count)\n",
    "        test_data = balance_train_data(test_data, int(count / 10))\n",
    "        data = pd.concat([train_data, dev_data, test_data], axis=0)\n",
    "        return data\n",
    "\n",
    "    data = get_dataset()\n",
    "    l = len(data)\n",
    "    train_num = int(l*0.7)\n",
    "    dev_num = int(l*0.1)\n",
    "    rnd_ind = np.arange(l)\n",
    "    np.random.shuffle(rnd_ind)\n",
    "    train_ind = rnd_ind[:train_num]\n",
    "    dev_ind = rnd_ind[train_num:train_num+dev_num]\n",
    "    test_ind = rnd_ind[train_num+dev_num:]\n",
    "    data = data.reset_index()\n",
    "    data['ind'] = data.index\n",
    "\n",
    "    def change_phase(ind):\n",
    "        if ind in train_ind:\n",
    "            return 'train'\n",
    "        elif ind in dev_ind:\n",
    "            return 'dev'\n",
    "        else:\n",
    "            return 'test'\n",
    "\n",
    "    data['phase'] = data['ind'].apply(change_phase)\n",
    "    data = data.drop(columns=['ind'])\n",
    "\n",
    "    data.to_csv(TRAIN_TEST_SPLIT_FILE, index=False)\n",
    "\n",
    "    train_data = data[data['phase']=='train']\n",
    "    test_data = data[data['phase']=='test']\n",
    "    dev_data = data[data['phase']=='dev']\n",
    "    train_data = balance_train_data(train_data, count)\n",
    "    test_data = balance_train_data(test_data, int(count / 10))\n",
    "    data = pd.concat([train_data, dev_data, test_data], axis=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_balanced_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_balanced_train_test_split(count=10000)\n",
    "\n",
    "train = df.loc[df['phase'] == 'train']\n",
    "dev = df.loc[df['phase'] == 'dev']\n",
    "test = df.loc[df['phase'] == 'test']\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "y = lbl_enc.fit_transform(train.category.values)\n",
    "\n",
    "X_train = train.headline.values\n",
    "X_valid = dev.headline.values\n",
    "X_test = test.headline.values\n",
    "\n",
    "Y_train = lbl_enc.transform(train.category.values)\n",
    "Y_valid = lbl_enc.transform(dev.category.values)\n",
    "y_test_true = lbl_enc.transform(test.category.values)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Transform y labels from 0, 1, 2 to one-hot encoding\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "\n",
    "Y_valid = np_utils.to_categorical(Y_valid)\n",
    "\n",
    "# Perform preprocessing for x and transform to np array\n",
    "X_train = [preprocess_X(x) for x in tqdm(X_train)]\n",
    "X_train = np.array(X_train)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_valid = [preprocess_X(x) for x in tqdm(X_valid)]\n",
    "X_valid = np.array(X_valid)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(300, input_dim=300, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(300, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y=Y_train, batch_size=64,\n",
    "          epochs=100, verbose=1, validation_data=(X_valid, Y_valid), callbacks=[es])\n",
    "\n",
    "X_test = [preprocess_X(x) for x in tqdm(X_test)]\n",
    "X_test = np.array(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test)\n",
    "print(y_test_pred)\n",
    "print(accuracy_score(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_test_true, y_test_pred,average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lbl_enc.inverse_transform(y_test_pred)\n",
    "\n",
    "test = test.assign(preds = y_test_pred)\n",
    "\n",
    "test.to_csv(\"simpleNN_balanced.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248",
   "language": "python",
   "name": "cs4248"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
